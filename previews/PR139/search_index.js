var documenterSearchIndex = {"docs":
[{"location":"tutorials/spd/","page":"Symmetric and positive definite linear systems","title":"Symmetric and positive definite linear systems","text":"using QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 1, 1, 1, 2, 3, 3, 4, 4, 5]\njcn = [1, 3, 4, 5, 2, 3, 5, 4, 5, 5]\nval = [53.0, 8.0, 4.0, 3.0, 10.0, 6.0, 8.0, 26.0, 5.0, 14.0]\n\nA = Symmetric(sparse(irn, jcn, val, 5, 5), :U)\nb = [108.0, 20.0, 66.0, 133.0, 117.0]\nx_star = [1.0, 2.0, 3.0, 4.0, 5.0]\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nx = qrm_spposv(spmat, b)\n\nerror_norm = norm(x - x_star)\nresidual_norm = norm(A * x - b)\n\n@printf(\"Error norm ‖x* - x‖ = %10.5e\\n\", error_norm)\n@printf(\"Residual norm ‖Ax - b‖ = %10.5e\\n\", residual_norm)\n\nusing QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 2, 1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5]\njcn = [1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5]\nval = [4.0, 1.0, 1.0, 4.0, 1.0, 1.0, 4.0, 1.0, 1.0, 4.0, 1.0, 1.0, 4.0]\n\nA = sparse(irn, jcn, val, 5, 5)\nA_L = tril(A)\nb = [5.0, 6.0, 6.0, 6.0, 5.0]\nx_star = [1.0, 1.0, 1.0, 1.0, 1.0]\n\nqrm_init()\n\nspmat = qrm_spmat_init(A_L, sym=true)\nx = qrm_spposv(spmat, b)\n\nerror_norm = norm(x - x_star)\nresidual_norm = norm(A * x - b)\n\n@printf(\"Error norm ‖x* - x‖ = %10.5e\\n\", error_norm)\n@printf(\"Residual norm ‖Ax - b‖ = %10.5e\\n\", residual_norm)\n\nusing QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 3, 4, 5, 2, 3, 5, 4, 5, 5]\njcn = [1, 1, 1, 1, 2, 3, 3, 4, 4, 5]\nval = [53.0, 8.0, 4.0, 3.0, 10.0, 6.0, 8.0, 26.0, 5.0, 14.0]\n\nA = Symmetric(sparse(irn, jcn, val, 5, 5), :L)\nb = [108.0, 20.0, 66.0, 133.0, 117.0]\nx_star = [1.0, 2.0, 3.0, 4.0, 5.0]\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\nqrm_analyse!(spmat, spfct)\nqrm_factorize!(spmat, spfct)\nz = qrm_solve(spfct, b, transp='t')\nx = qrm_solve(spfct, z)\n\nerror_norm = norm(x - x_star)\nresidual_norm = norm(A * x - b)\n\n@printf(\"Error norm ‖x* - x‖ = %10.5e\\n\", error_norm)\n@printf(\"Residual norm ‖Ax - b‖ = %10.5e\\n\", residual_norm)\n\nusing QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 2, 1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5]\njcn = [1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5]\nval = [4.0, 1.0, 1.0, 4.0, 1.0, 1.0, 4.0, 1.0, 1.0, 4.0, 1.0, 1.0, 4.0]\n\nA = sparse(irn, jcn, val, 5, 5)\nA_U = triu(A)\nb = [5.0, 6.0, 6.0, 6.0, 5.0]\nx_star = [1.0, 1.0, 1.0, 1.0, 1.0]\n\nqrm_init()\n\nspmat = qrm_spmat_init(A_U, sym=true)\nspfct = qrm_spfct_init(spmat)\n\nqrm_analyse!(spmat, spfct)\nqrm_factorize!(spmat, spfct)\nz = qrm_solve(spfct, b, transp='t')\nx = qrm_solve(spfct, z)\n\nerror_norm = norm(x - x_star)\nresidual_norm = norm(A * x - b)\n\n@printf(\"Error norm ‖x* - x‖ = %10.5e\\n\", error_norm)\n@printf(\"Residual norm ‖Ax - b‖ = %10.5e\\n\", residual_norm)\n\nusing QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 2, 2, 3, 3, 3]\njcn = [1, 1, 2, 1, 2, 3]\nval = [7.0, -im, 8.0, 5im, 5.0, 10.0]\n\nA = Hermitian(sparse(irn, jcn, val, 3, 3), :L)\nb = [11.0-6im, 32.0+12im, 35.0+20im]\nx_star = [1.0+im, 2.0+im, 3.0+im]\nx = copy(b)\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\nqrm_analyse!(spmat, spfct)\nqrm_factorize!(spmat, spfct)\n\nqrm_solve!(spfct, x, x, transp='c')\nqrm_solve!(spfct, x, x)\n\nerror_norm = norm(x - x_star)\nresidual_norm = norm(A * x - b)\n\n@printf(\"Error norm ‖x* - x‖ = %10.5e\\n\", error_norm)\n@printf(\"Residual norm ‖Ax - b‖ = %10.5e\\n\", residual_norm)\n\nusing QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 1, 1, 2, 2, 3]\njcn = [1, 2, 3, 2, 3, 3]\nval = [7.0, im, -5im, 8.0, 5.0, 10.0]\n\nA = Hermitian(sparse(irn, jcn, val, 3, 3), :U)\nb = [11.0-6im, 32.0+12im, 35.0+20im]\nx_star = [1.0+im, 2.0+im, 3.0+im]\nx = copy(b)\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\nqrm_analyse!(spmat, spfct)\nqrm_factorize!(spmat, spfct)\n\nqrm_solve!(spfct, x, x, transp='c')\nqrm_solve!(spfct, x, x)\n\nerror_norm = norm(x - x_star)\nresidual_norm = norm(A * x - b)\n\n@printf(\"Error norm ‖x* - x‖ = %10.5e\\n\", error_norm)\n@printf(\"Residual norm ‖Ax - b‖ = %10.5e\\n\", residual_norm)","category":"section"},{"location":"tutorials/ls/","page":"Least-squares problems","title":"Least-squares problems","text":"using QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 1, 1, 2, 3, 3, 4, 4, 5, 5, 6, 7, 7]\njcn = [1, 3, 5, 2, 3, 5, 1, 4, 4, 5, 2, 1, 3]\nval = [1.0, 2.0, 3.0, 1.0, 1.0, 2.0, 4.0, 1.0, 5.0, 1.0, 3.0, 6.0, 1.0]\n\nA = sparse(irn, jcn, val, 7, 5)\nb = [22.0, 5.0, 13.0, 8.0, 25.0, 5.0, 9.0]\nx_star = [1.0, 2.0, 3.0, 4.0, 5.0]\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nx = qrm_least_squares(spmat, b)\n\nerror_norm = norm(x - x_star)\nr = A * x - b\noptimality_residual_norm = norm(A' * r)\n\n@printf(\"Error norm ‖x* - x‖ = %10.5e\\n\", error_norm)\n@printf(\"Optimality residual norm ‖Aᵀr‖ = %10.5e\\n\", optimality_residual_norm)\n\nusing QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 1, 1, 2, 3, 3, 4, 4, 5, 5, 6, 7, 7]\njcn = [1, 3, 5, 2, 3, 5, 1, 4, 4, 5, 2, 1, 3]\nval = [1.0, 2.0, 3.0, 1.0, 1.0, 2.0, 4.0, 1.0, 5.0, 1.0, 3.0, 6.0, 1.0]\n\nA = sparse(irn, jcn, val, 7, 5)\nb = [22.0, 5.0, 13.0, 8.0, 25.0, 5.0, 9.0]\nx_star = [1.0, 2.0, 3.0, 4.0, 5.0]\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\nqrm_analyse!(spmat, spfct)\nqrm_factorize!(spmat, spfct)\nz = qrm_apply(spfct, b, transp='t')\nx = qrm_solve(spfct, z)\n\nerror_norm = norm(x - x_star)\nr = A * x - b\noptimality_residual_norm = norm(A' * r)\n\n@printf(\"Error norm ‖x* - x‖ = %10.5e\\n\", error_norm)\n@printf(\"Optimality residual norm ‖Aᵀr‖ = %10.5e\\n\", optimality_residual_norm)\n\nusing QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 1, 1, 2, 3, 3, 4, 4, 5, 5, 6, 7, 7]\njcn = [1, 3, 5, 2, 3, 5, 1, 4, 4, 5, 2, 1, 3]\nval = [1.0+im, 2.0-im, 3.0+im, 1.0-im, 1.0+im, 2.0-im, 4.0+im, 1.0-im, 5.0+im, 1.0-im, 3.0+im, 6.0-im, 1.0+im]\n\nA = sparse(irn, jcn, val, 7, 5)\nb = [1.0+im, 2.0+im, 3.0+im, 4.0+im, 5.0+im, 6.0+im, 7.0+im]\nz = copy(b)\nx = zeros(ComplexF64, 5)\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\nqrm_analyse!(spmat, spfct)\nqrm_factorize!(spmat, spfct)\nqrm_apply!(spfct, z, transp='c')\nqrm_solve!(spfct, z, x)\n\nr = A * x - b\noptimality_residual_norm = norm(A' * r)\n\n@printf(\"Optimality residual norm ‖Aᵀr‖ = %10.5e\\n\", optimality_residual_norm)","category":"section"},{"location":"performance/#Performance-tuning","page":"Performance tuning","title":"Performance tuning","text":"The performance of qr_mumps depends on a number of parameters. Default values are provided for these parameters that are expected to achieve reasonably good performance on a wide range of problems and architectures but for optimal performance these should be tuned. In this section we provide a list of these parameters and explain how do they have an effect on performance.","category":"section"},{"location":"performance/#**Block-size**","page":"Performance tuning","title":"Block size","text":"qr_mumps decomposes frontal matrices into blocks of size mb × mb (set through the qrm_mb control parameter); this decomposition provides an additional level of parallelism (other than that already expressed by the elimination tree) because it is possible to execute concurrently tasks that operate on different blocks. On the one hand, small values of mb provide high parallelism; on the other hand, high values of mb provide high efficiency for each task and make the tasks scheduling overhead negligible. This parameter should be, therefore, chosen as to provide the best compromise between parallelism and tasks efficiency. The optimal value depends on the size and structure of the problem, the number and features of processing units, the efficiency and scalability of BLAS operations, etc... On current CPUs block sizes of 128 or 256 achieve close to optimal task performance and good parallelism on moderately sized problems; if GPUs are used, higher block sizes (1024) provide better performance. Choosing a large mb value to achieve high performance on GPU devices can severely reduce parallelism and lead to CPU starvation. In this case the nb parameter (qrm_nb) can be used to generate additional parallelism; if this parameter is set to a submultiple of mb, the dynamic, hierarchical partitioning technique is used which can lead to better performance. Finally, some tasks use an internal block size; this is set by the ib parameter (qrm_ib which has to be a submultiple of mb and nb) and defines a compromise between efficiency of tasks and overall amount of floating point operations. Again, when GPUs are used, larger values of ib lead to better speed whereas on CPUs values of 32/64 provide satisfactory speed.","category":"section"},{"location":"performance/#**Reduction-tree-shape**","page":"Performance tuning","title":"Reduction tree shape","text":"The bh parameter (qrm_bh) defines the shape of the reduction tree in the QR panel reduction. A value of k means that a panel is divided in groups of size k, intra-group reduction is done with a flat tree, inter-group reduction with a binary tree. Therefore, a value of one achieves the highest parallelism because the whole panel is reduced through a binary tree. Conversely a value which is equal or higher than the number of blocks in a panel leads to lower parallelism because all the blocks in the panels are reduced one after the other; a zero or negative value sets a flat tree on all panels in all fronts of the multifrontal factorization. Nevertheless it must be noted that excessively small values of bh may lead to inefficient computations because of the nature of the involved tasks. A flat tree typically achieves high performance on a wide range of problems but for very overdetermined problems it may be beneficial to use hybrid trees.","category":"section"},{"location":"performance/#**Ordering**","page":"Performance tuning","title":"Ordering","text":"Fill-reducing ordering is essential to limit the fill-in produced by the factorization. This ordering (set through the qrm_ordering control parameter) is computed during the analysis phase and corresponds to a matrix permutation that defines the order in which unknowns are eliminated. The ordering will also affect the shape of the elimination tree which can be more or less balanced or deep with obvious consequences on parallelism, efficiency and, ultimately, execution time. Nested Dissection methods, such as those implemented in the Metis and SCOTCH packages, usually provide the best results and their running time may be high; local orderings such as AMD/COLAMD typically have a lower running time, which results in a faster analysis step, but lead to higher fill-in and thus higher running time and memory consumption for the factorization and the solve.","category":"section"},{"location":"tutorials/rank_deficiency/","page":"Detect rank deficiency","title":"Detect rank deficiency","text":"using QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4]\njcn = [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]\nval = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0]\n\nA = sparse(irn, jcn, val, 4, 3)\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\n# The control parameter `qrm_rd_eps` is a threshold to estimate the rank of the problem.\n# If qrm_rd_eps > 0 the qrm_factorize routine will count the number of diagonal\n# coefficients of the R factor whose absolute value is smaller than the provided value.\nqrm_set(spfct, \"qrm_rd_eps\", 1e-12)\n\n# Perform the analysis and factorization phases\nqrm_analyse!(spmat, spfct)\nqrm_factorize!(spmat, spfct)\n\nqrm_get(spfct, \"qrm_rd_eps\")\n\n# The information parameter `qrm_rd_num` contains the number of diagonal coefficients\n# of the R factor whose absolute value is lower than `qrm_rd_eps`.\nrank_deficiency = qrm_get(spfct, \"qrm_rd_num\")\n\nusing QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]\njcn = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]\nval = [1.0, 2.0, 3.0, 6.0, 4.0, 5.0, 6.0, 15.0, 7.0, 8.0, 9.0, 24.0]\n\nA = sparse(irn, jcn, val, 3, 4)\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\n# The control parameter `qrm_rd_eps` is a threshold to estimate the rank of the problem.\n# If qrm_rd_eps > 0 the qrm_factorize routine will count the number of diagonal\n# coefficients of the R factor whose absolute value is smaller than the provided value.\nqrm_set(spfct, \"qrm_rd_eps\", 1e-12)\n\n# Perform the analysis and factorization phases\nqrm_analyse!(spmat, spfct, transp='t')\nqrm_factorize!(spmat, spfct, transp='t')\n\n# The information parameter `qrm_rd_num` contains the number of diagonal coefficients\n# of the R factor whose absolute value is lower than `qrm_rd_eps`.\nrank_deficiency = qrm_get(spfct, \"qrm_rd_num\")","category":"section"},{"location":"information_parameters/#Information-parameters","page":"Information parameters","title":"Information parameters","text":"Information parameters return information about the behavior of qr_mumps and can be either global or problem specific. All the information parameters can be gotten through the qrm_get routine; problem specific control parameters can also be retrieved by manually reading the gstats attribute of a qrm_spfct factorization. The qrm_get routine can also be used to retrieve the values of all the control parameters described in the previous section with the obvious usage. The type of all information parameters is Int64.","category":"section"},{"location":"information_parameters/#Global-parameters","page":"Information parameters","title":"Global parameters","text":"qrm_max_mem: this parameter returns the maximum amount of memory allocated by qr_mumps during its execution.\n\nqrm_tot_mem: this parameter returns the total amount of memory allocated by qr_mumps at the moment when the qrm_get routine is called.","category":"section"},{"location":"information_parameters/#Problem-specific-parameters","page":"Information parameters","title":"Problem specific parameters","text":"qrm_e_facto_flops: this parameter returns an estimate, computed during the analysis phase, of the number of floating point operations performed during the factorization phase. This value is only available after the qrm_analyse routine is executed.\n\nqrm_facto_flops: this parameter returns the number of floating point operations performed during the factorization phase. This value is only available after the qrm_analyse routine is executed.\n\nqrm_e_nnz_r: this parameter returns an estimate, computed during the analysis phase, of the number of nonzero coefficients in the R factor. This value is only available after the qrm_analyse routine is executed.\n\nqrm_nnz_r: this parameter returns the actual number of the nonzero coefficients in the R factor after the factorization is done. This value is only available after the qrm_factorize routine is executed.\n\nqrm_e_nnz_h: this parameter returns an estimate, computed during the analysis phase, of the number of nonzero coefficients in the Q matrix. This value is only available after the qrm_analyse routine is executed.\n\nqrm_nnz_h: this parameter returns the actual number of the nonzero coefficients in the Q matrix after the factorization is done. This value is only available after the qrm_factorize routine is executed.\n\nqrm_e_facto_mempeak: this parameter returns an estimate of the peak memory consumption of the factorization operation.\n\nqrm_rd_num: this information parameter returns the number of diagonal coefficients of the R factor whose absolute value is lower than qrm_rd_eps if this control parameter was set to a value greater than 0.","category":"section"},{"location":"features/#Features","page":"Features","title":"Features","text":"","category":"section"},{"location":"features/#**Types-of-problems**","page":"Features","title":"Types of problems","text":"qr_mumps can handle unsymmetric and symmetric, positive definite problems. In the first case it will use a QR factorization whereas, in the second, it will use a Cholesky factorization. In order to choose one or the other method, qr_mumps must be informed about the type of the problem through the sym argument of the qrm_spmat_init function: false means that the problem is unsymmetric and true means symmetric, positive definite. Note that in the second case, only half of the matrix must be provided, i.e., if the coefficient (i, j) is provided (j, i) must not be given.","category":"section"},{"location":"features/#**Memory-consumption-control**","page":"Features","title":"Memory consumption control","text":"qr_mumps allows for controlling the amount of memory used in the parallel factorization stage. In the multifrontal method, the memory consumption varies greatly throughout the sequential factorization reaching a maximum value which is referred to as the sequential peak (sp). Parallelism can considerably increase this peak because, in order to feed the working threads, more data is allocated at the same time which results in higher concurrency. In qr_mumps it is possible to bound the memory consumption of the factorization phase through the qrm_mem_relax parameter. If this parameter is set to a real value x ≥ 1, the memory consumption will be bounded by x × sp. Clearly, the tighter is this upper bound, the slower the factorization will proceed. Note that sp only includes the memory consumed by the factorization operation; moreover, although in practice it is possible to precisely pre-compute this value in the analysis phase, this may be expensive and thus qrm_analyse only computes a (hopefully) slight overestimation. The value of sp is available upon completion of the analysis phase through the qrm_e_facto_mempeak information parameter.","category":"section"},{"location":"features/#**Fill-reducing-permutations**","page":"Features","title":"Fill-reducing permutations","text":"qr_mumps supports multiple methods for reducing the factorization fill-in through matrix column permutations. The choice is controlled through the qrm_ordering control parameter. Nested-dissection based methods are available through the packages Metis and SCOTCH packages as well as average minimum degree through the COLAMD one. Nested-dissection based methods usually lead to lower fill-in which ultimately results in faster and less memory consuming factorization. COLAMD, instead, typically leads to a faster execution of the analysis phase although is not as effective in reducing the fill-in which may result in a slower and more memory consuming factorization. Because the overall execution time is commonly dominated by the factorization, nested-dissection methods are usually more effective especially for large size problems. qr_mumps also allows the user to provide their own permutation.","category":"section"},{"location":"control_parameters/#Control-parameters","page":"Control parameters","title":"Control parameters","text":"Control parameters define the behavior of qr_mumps and can be set in two modes:\n\nglobal mode: in this mode it possible to either set generic parameters (e.g., the unit for output or error messages) or default parameter values (e.g., the ordering method to be used on the problem) that apply to all initialized qrm_spfct factorizations.\n\nproblem mode: these parameters control the behavior of qr_mumps on a specific sparse factorization problem. Because the qrm_spfct_init routine sets the control parameters to their default values, these have to be modified after the sparse factorization object initialization.\n\nAll the control parameters can be set through the qrm_set routine.","category":"section"},{"location":"control_parameters/#Global-parameters","page":"Control parameters","title":"Global parameters","text":"qrm_ncpu: integer specifying the number of CPU cores to use for the subsequent qr_mumps calls. It is an argument to the qrm_init routine. Default is 1.\n\nqrm_ounit: integer specifying the unit for output messages; if negative, output messages are suppressed. Default is 6 (stdout).\n\nqrm_eunit: an integer specifying the unit for error messages; if negative, error messages are suppressed. Default is 0.","category":"section"},{"location":"control_parameters/#Problem-specific-parameters","page":"Control parameters","title":"Problem specific parameters","text":"qrm_ordering: this parameter specifies what permutation to apply to the columns of the input matrix in order to reduce the fill-in and, consequently, the operation count of the factorization and solve phases. This parameter is used by qr_mumps during the analysis phase and, therefore, has to be set before it starts. The following pre-defined values are accepted:\nqrm_auto (0) : the choice is automatically made by qr_mumps. This is the default.\nqrm_natural (1) : no permutation is applied.\nqrm_given (2) : a column permutation is provided by the user through the cperm_in attribute of a qrm_spfct factorization.\nqrm_colamd (3) : the COLAMD software package (if installed) is used for computing the column permutation.\nqrm_metis (4) : the Metis software package (if installed) is used for computing the column permutation.\nqrm_scotch (5) : the SCOTCH software package (if installed) is used for computing the column permutation.\n\nqrm_keeph: this parameter says whether the Q matrix should be kept for later use or discarded. This parameter is used by qr_mumps during the factorization phase and, therefore, has to be set before it starts. Accepted value are:\nqrm_yes (1) : the Q matrix is kept. This is the default.\nqrm_no (0) : the Q matrix is discarded.\n\nqrm_mb and qrm_nb: These parameters define the block-size (rows and columns, respectively) for data partitioning and, thus, granularity of parallel tasks. Smaller values mean higher concurrence. This parameter, however, implicitly defines an upper bound for the granularity of call to BLAS and LAPACK routines (defined by the qrm_ib parameter described below); therefore, excessively small values may result in poor performance. This parameter is used by qr_mumps during the analysis and factorization phases and, therefore, has to be set before these start. The default value is 256 for both. Note that qrm_mb has to be a multiple of qrm_nb.\n\nqrm_ib: this parameter defines the granularity of BLAS/LAPACK operations. Larger values mean better efficiency but imply more fill-in and thus more flops and memory consumption. The value of this parameter is upper-bounded by the qrm_nb parameter described above. This parameter is used by qr_mumps during the factorization phase and, therefore, has to be set before it starts. The default value is 32. It is strongly advised to choose, for this parameter, a submultiple of qrm_nb.\n\nqrm_bh: this parameter defines the type of algorithm for the communication-avoiding QR factorization of frontal matrices. Smaller values mean more concurrency but worse tasks efficiency; if lower or equal to zero the largest possible value is chosen for each front. Default value is -1.\n\nqrm_rhsnb: in the case where multiple right-hand sides are passed to the qrm_apply or the qrm_solve routines, this parameter can be used to define a blocking of the right-hand sides. This parameter is used by qr_mumps during the solve phase and, therefore, has to be set before it starts. By default, all the right-hand sides are treated in a single block.\n\nqrm_mem_relax: a value (≥ 1) that sets a relaxation parameter, with respect to the sequential peak, for the memory consumption in the factorization phase. If negative, the memory consumption is not bounded. Default value is −1.0.\n\nqrm_rd_eps: a value setting a threshold to estimate the rank of the problem. If > 0 the qrm_factorize routine will count the number of diagonal coefficients of the R factor whose absolute value is smaller than the provided value. This number can be retrieved through the qrm_rd_num information parameter described in the next section.","category":"section"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#QRMumps.qrm_spmat","page":"API","title":"QRMumps.qrm_spmat","text":"This data type is used to store a sparse matrix in the COO (or coordinate) format through the irn, jcn and val fields containing the row indices, column indices and values, respectively and the m, n and nz containing the number of rows, columns and nonzeros, respectively. qr_mumps uses a Fortran-style 1-based numbering and thus all row indices are expected to be between 1 and m and all the column indices between 1 and n. Duplicate entries are summed during the factorization, out-of-bound entries are ignored. The sym field is used to specify if the matrix is symmetric and positive definite (true) or not (false).\n\n\n\n\n\n","category":"type"},{"location":"api/#QRMumps.qrm_spfct","page":"API","title":"QRMumps.qrm_spfct","text":"This type is used to set the parameters that control the behavior of a sparse factorization, to collect information about its execution (number of flops, memory consumpnion etc) and store the result of  the factorization, namely, the factors with all the symbolic information needed to use them in the solve phase.\n\n\n\n\n\n","category":"type"},{"location":"api/#QRMumps.qrm_init","page":"API","title":"QRMumps.qrm_init","text":"qrm_init(ncpu, ngpu)\n\nThis routine initializes qr_mumps and should be called prior to any other qr_mumps routine. This function is automatically called if you use qr_mumps precompiled with Yggdrasil.\n\nqrm_init()\n\nncpu and ngpu are optional arguments and their default value are, respectively, 1 and 0.\n\nInput Arguments :\n\nncpu: number of working threads on CPU.\nngpu: number of working threads on GPU.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_finalize","page":"API","title":"QRMumps.qrm_finalize","text":"qrm_finalize()\n\nThis routine finalizes qr_mumps and no other qr_mumps routine should be called afterwards.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spmat_init","page":"API","title":"QRMumps.qrm_spmat_init","text":"spmat = qrm_spmat_init(A; sym=false)\nspmat = qrm_spmat_init(m, n, rows, cols, vals; sym=false)\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spmat_init!","page":"API","title":"QRMumps.qrm_spmat_init!","text":"qrm_spmat_init!(spmat, A; sym=false)\nspmat = qrm_spmat_init!(spmat, m, n, rows, cols, vals; sym=false)\n\nThis routine initializes a qrm_spmat type data structure from a sparseMatrixCSC.\n\nInput Arguments :\n\nIn the first form,\n\nspmat: the qrm_spmat sparse matrix to be initialized.\nA : a Julia sparse matrix stored in either SparseMatrixCSC or SparseMatrixCOO format (see SparseMatricesCOO.jl for the second case).\nsym : a boolean to specify if the matrix is symmetric / hermitian (true) or unsymmetric (false).\n\nIn the second form, the matrix A is specified using\n\nm: the number of rows.\nn: the number of columns.\nrows: the array of row indices of nonzero elements.\ncols: the array of column indices of nonzero elements.\nvals: the array of values of nonzero elements.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spmat_destroy!","page":"API","title":"QRMumps.qrm_spmat_destroy!","text":"qrm_spmat_destroy!(spmat)\n\nThis routine cleans up a qrm_spmat type data structure.\n\nInput Argument :\n\nspfct: the sparse matrix to be destroyed.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spfct_init","page":"API","title":"QRMumps.qrm_spfct_init","text":"spfct = qrm_spfct_init(spmat)\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spfct_init!","page":"API","title":"QRMumps.qrm_spfct_init!","text":"qrm_spfct_init!(spmat, spfct)\n\nThis routine initializes a qrm_spfct type data structure. This amounts to setting all the control parameters to the default values.\n\nInput Arguments :\n\nspmat: the input matrix of type qrm_spmat.\nspfct: the sparse factorization object to be initialized.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spfct_destroy!","page":"API","title":"QRMumps.qrm_spfct_destroy!","text":"qrm_spfct_destroy!(spfct)\n\nThis routine cleans up a qrm_spfct type data structure by deleting the result of a sparse factorization.\n\nInput Argument :\n\nspfct: the sparse factorization object to be destroyed.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_analyse","page":"API","title":"QRMumps.qrm_analyse","text":"spfct = qrm_analyse(spmat; transp='n')\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_analyse!","page":"API","title":"QRMumps.qrm_analyse!","text":"qrm_analyse!(spmat, spfct; transp='n')\n\nThis routine performs the analysis phase and updates spfct.\n\nInput Arguments :\n\nspmat: the input matrix of type qrm_spmat.\nspfct: the sparse factorization object of type qrm_spfct.\ntransp: whether the input matrix should be transposed or not. Can be either 't', 'c' or 'n'.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_update!","page":"API","title":"QRMumps.qrm_update!","text":"qrm_update!(spmat, A)\nqrm_update!(spmat, vals)\n\nThis routine updates a qrm_spmat type data structure from a sparseMatrixCSC. spmat and A must have the same sparsity pattern.\n\nInput Arguments :\n\nIn the first form,\n\nspmat: the qrm_spmat sparse matrix to be updated.\nA : a Julia sparse matrix stored in SparseMatrixCSC format.\n\nIn the second form,\n\nvals: the array of values of nonzero elements of A.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_factorize!","page":"API","title":"QRMumps.qrm_factorize!","text":"qrm_factorize!(spmat, spfct; transp='n')\n\nThis routine performs the factorization phase. It can only be executed if the analysis is already done.\n\nInput Arguments :\n\nspmat: the input matrix of type qrm_spmat.\nspfct: the sparse factorization object of type qrm_spfct.\ntransp: whether the input matrix should be transposed or not. Can be either 't', 'c' or 'n'.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_solve","page":"API","title":"QRMumps.qrm_solve","text":"x = qrm_solve(spfct, b; transp='n')\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_solve!","page":"API","title":"QRMumps.qrm_solve!","text":"qrm_solve!(spfct, b, x; transp='n')\n\nThis routine solves the triangular system Rx = b or Rᵀx = b. It can only be executed once the factorization is done.\n\nInput Arguments :\n\nspfct: the sparse factorization object resulting from the qrm_factorize! function.\nb: the right-hand side(s).\nx: the solution vector(s).\ntransp: whether to solve for R, Rᵀ or Rᴴ. Can be either 't', 'c' or 'n'.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_apply","page":"API","title":"QRMumps.qrm_apply","text":"z = qrm_apply(spfct, b; transp='n')\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_apply!","page":"API","title":"QRMumps.qrm_apply!","text":"qrm_apply!(spfct, b; transp='n')\n\nThis routine computes z = Qb, z = Qᵀb or z = Qᴴb in place and overwrites b. It can only be executed once the factorization is done.\n\nInput Arguments :\n\nspfct: the sparse factorization object resulting from the qrm_factorize! function.\nb: the vector(s) to which Q, Qᵀ or Qᴴ is applied.\ntransp: whether to apply Q, Qᵀ or Qᴴ. Can be either 't', 'c' or 'n'.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spfct_get_cp","page":"API","title":"QRMumps.qrm_spfct_get_cp","text":"cp = qrm_spfct_get_cp(spfct)\n\nReturns the column permutation.\n\nInput Arguments :\n\nspfct: a sparse factorization object of type qrm_spfct.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spfct_get_rp","page":"API","title":"QRMumps.qrm_spfct_get_rp","text":"rp = qrm_spfct_get_rp(spfct)\n\nReturns the row permutation.\n\nInput Arguments :\n\nspfct: a sparse factorization object of type qrm_spfct.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spfct_get_r","page":"API","title":"QRMumps.qrm_spfct_get_r","text":"R = qrm_spfct_get_r(spfct)\n\nReturns the R factor as a SparseMatrixCSC matrix.\n\nInput Arguments :\n\nspfct: a sparse factorization object of type qrm_spfct.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spbackslash","page":"API","title":"QRMumps.qrm_spbackslash","text":"x = qrm_spbackslash(spmat, b; transp='n')\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spbackslash!","page":"API","title":"QRMumps.qrm_spbackslash!","text":"qrm_spbackslash!(spmat, b, x; transp='n')\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spposv","page":"API","title":"QRMumps.qrm_spposv","text":"x = qrm_spposv(spmat, b)\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spposv!","page":"API","title":"QRMumps.qrm_spposv!","text":"qrm_spposv!(spmat, b, x)\n\nThis function can be used to solve a linear symmetric, positive definite problem. It is a shortcut for the sequence\n\nx = b\nqrm_analyse!(spmat, spfct; transp='n')\nqrm_factorize!(spmat, spfct; transp='n')\nqrm_solve!(spfct, x, x; transp='t')\nqrm_solve!(spfct, x, x; transp='t')\n\nInput Arguments :\n\nspmat: the input matrix.\nb: the right-hand side(s).\nx: the solution vector(s).\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_least_squares","page":"API","title":"QRMumps.qrm_least_squares","text":"x = qrm_least_squares(spmat, b; transp='n')\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_least_squares!","page":"API","title":"QRMumps.qrm_least_squares!","text":"qrm_least_squares!(spmat, b, x; transp='n')\n\nThis function can be used to solve a linear least squares problem\n\nmin Ax  b_2\n\nin the case where the input matrix is square or overdetermined. It is a shortcut for the sequence\n\nqrm_analyse!(spmat, spfct; transp='n')\nqrm_factorize!(spmat, spfct; transp='n')\nqrm_apply!(spfct, b; transp='t')\nqrm_solve!(spfct, b, x; transp='n')\n\nInput Arguments :\n\nspmat: the input matrix.\nb: the ight-hand side(s).\nx: the solution vector(s).\ntransp: whether to use A, Aᵀ or Aᴴ. Can be either 't', 'c' or 'n'.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_least_squares_semi_normal","page":"API","title":"QRMumps.qrm_least_squares_semi_normal","text":"x = qrm_least_squares_semi_normal(spmat, b)\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_least_squares_semi_normal!","page":"API","title":"QRMumps.qrm_least_squares_semi_normal!","text":"qrm_least_squares_semi_normal!(spmat, b, x, z, Δx, y; transp='n')\n\nThis function can be used to solve a linear least squares problem\n\nmin Ax  b_2\n\nin the case where A is square or overdetermined. Contrary to qrm_least_squares!, this function allows to solve the problem without storing the Q-factor of the QR factorization of A.\n\nIt is a shortcut for the sequence\n\nqrm_set(spfct, \"qrm_keeph\", 0)\nqrm_analyse!(spmat, spfct, transp  = 'n')\nqrm_factorize!(spmat, spfct, transp = 'n')\nqrm_spmat_mv!(spmat, T(1), b, T(0), z, transp = 't')\nqrm_solve!(spfct, z, y, transp = 't')\nqrm_solve!(spfct, y, x, transp = 'n')\nqrm_refine!(spmat, spfct, x, z, Δx, y)\n\nNote that the Q-factor is not used in this sequence; only A and R. \n\nInput Arguments\n\nspmat: the input matrix.\nspfct: a sparse factorization object of type qrm_spfct.\nb: the right-hand side(s).\nx: the solution vector(s).\nΔx: an auxiliary vector (or matrix if b and x are matrices) used to compute the solution, the size of this vector (resp. matrix) is the same as x and can be uninitialized when the function is called.\nz: an auxiliary vector (or matrix if b and x are matrices) used to store the value z = Aᵀb, the size of this vector (resp. matrix) is the same as x and can be uninitialized when the function is called.\ny: an auxiliary vector (or matrix if b and x are matrices) used to compute the solution, the size of this vector (resp. matrix) is the same as b and can be uninitialized when the function is called.\ntransp: whether to use A, Aᵀ or Aᴴ. Can be either 't', 'c' or 'n'.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_min_norm","page":"API","title":"QRMumps.qrm_min_norm","text":"x = qrm_min_norm(spmat, b; transp='n')\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_min_norm!","page":"API","title":"QRMumps.qrm_min_norm!","text":"qrm_min_norm!(spmat, b, x; transp='n')\n\nThis function can be used to solve a linear minimum norm problem\n\nmin x_2 quad st quad Ax = b\n\nin the case where the input matrix is square or underdetermined. It is a shortcut for the sequence\n\nqrm_analyse!(spmat, spfct; transp='t')\nqrm_factorize!(spmat, spfct; transp='t')\nqrm_solve!(spfct, b, x; transp='t')\nqrm_apply!(spfct, x; transp='n')\n\nInput Arguments :\n\nspmat: the input matrix.\nb: the right-hand side(s).\nx: the solution vector(s).\ntransp: whether to use A, Aᵀ or Aᴴ. Can be either 't', 'c' or 'n'.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_min_norm_semi_normal","page":"API","title":"QRMumps.qrm_min_norm_semi_normal","text":"x = qrm_min_norm_semi_normal(spmat, b)\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_min_norm_semi_normal!","page":"API","title":"QRMumps.qrm_min_norm_semi_normal!","text":"qrm_min_norm_semi_normal!(spmat, spfct, b, x, Δx, y; transp='n')\n\nThis function can be used to solve a linear minimum-norm problem\n\nmin x_2 quad st quad Ax = b\n\nin the case where A is square or underdetermined. Contrary to qrm_min_norm!, this function allows to solve the problem without storing the Q-factor in the QR factorization of Aᵀ. It is a shortcut for the sequence\n\nqrm_set(spfct, \"qrm_keeph\", 0)\nqrm_analyse!(spmat, spfct, transp = 't')\nqrm_factorize!(spmat, spfct, transp = 't')\nqrm_solve!(spfct, b, Δx, transp = 't')\nqrm_solve!(spfct, Δx, y, transp = 'n')\nqrm_spmat_mv!(spmat, T(1),  y, T(0), x, transp = 't')\n\nRemark that the Q-factor is not used in this sequence but rather A and R. \n\nInput Arguments :\n\nspmat: the input matrix.\nspfct: a sparse factorization object of type qrm_spfct.\nb: the right-hand side(s).\nx: the solution vector(s).\nΔx: an auxiliary vector (or matrix if x and b are matrices) used to compute the solution, the size of this vector (resp. matrix) is the same as x and can be uninitialized when the function is called.\ny: an auxiliary vector (or matrix if x and b are matrices) used to compute the solution, the size of this vector (resp. matrix) is the same as b and can be uninitialized when the function is called.\ntransp: whether to use A, Aᵀ or Aᴴ. Can be either 't', 'c' or 'n'.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_refine","page":"API","title":"QRMumps.qrm_refine","text":"x_refined = qrm_refine(spmat, spfct, x, z)\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_refine!","page":"API","title":"QRMumps.qrm_refine!","text":"qrm_refine!(spmat, spfct, x, z, Δx, y)\n\nGiven an approximate solution x of the linear system RᵀRx ≈ z where R is the R-factor of some QR factorization of size (m, n), compute a refined solution.\n\nInput Arguments :\n\nspmat: the input matrix.\nspfct: a sparse factorization object of type qrm_spfct.\nx: the approximate solution vector or matrix, the size of this vector (resp. matrix) is n (resp. n×k).\nz: the RHS vector or matrix of the linear system, the size of this vector (resp. matrix) is n (resp. n×k).\nΔx: an auxiliary vector (or matrix if x and z are matrices) used to compute the refinement, the size of this vector (resp. matrix) is n (resp. n×k) and can be uninitialized when the function is called.\ny: an auxiliary vector (or matrix if x and z are matrices) used to compute the refinement, the size of this vector (resp. matrix) is m (resp. m×k) and can be uninitialized when the function is called.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_update_shift_spmat!","page":"API","title":"QRMumps.qrm_update_shift_spmat!","text":"qrm_update_shift_spmat!(shifted_spmat, α)\n\nGiven a shifted block matrix of the form (A  √α), update the parameter α in the matrix.\n\nInput Arguments\n\nshifted_spmat: the input matrix of the qrmshiftedspmat type. See qrm_shifted_spmat for more information.\nα ≥ 0: the regularization parameter (note the square root in the block matrix representation).\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_shift_spmat","page":"API","title":"QRMumps.qrm_shift_spmat","text":"qrm_shifted_spmat = qrm_shift_spmat(spmat, α = 0)\n\nGiven a spmat structure representing some sparse matrix A, return the block matrix (A  √α) as a qrm_shifted_spmat type. See qrm_shifted_spmat for more information. This can be especially useful when A is rank deficient, as choosing α > 0 acts as a regularization.\n\nInput Arguments\n\nspmat: the input matrix\nα ≥ 0: the regularization parameter (note the square root in the block matrix representation).\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_shifted_spmat","page":"API","title":"QRMumps.qrm_shifted_spmat","text":"This data type represents a \"shifted\" matrix. When one wants to solve a \"regularized\" problem of the form (AᵀA + αI)x = b, one can use a QR factorization of the block matrix QR = (A  √α)ᵀ and note that RᵀR = AᵀA + αI. This type of problem is especially useful when A is poorly conditioned or rank deficient. It only contains a qrm_spmat matrix representing the above block matrix and the regularization parameter α.\n\n\n\n\n\n","category":"type"},{"location":"api/#QRMumps.qrm_golub_riley!","page":"API","title":"QRMumps.qrm_golub_riley!","text":"qrm_golub_riley!(shifted_spmat, spfct, x, b, Δx, y; α = ϵm, max_iter = 50, tol = ϵm, transp = 'n')\n\nThis method implements the Golub-Riley iteration. Given a (possibly ill-conditionned or rank deficient) system Ax = b where A can have any shape m×n, compute x = A†b = Aᵀ(AAᵀ)†b where A† is the Moore-Penrose pseudoinverse of A.\n\nInput Arguments :\n\nshifted_spmat: a qrm_shifted_spmat type representing the matrix (A  √α) where α is a regularization parameter for the Golub-Riley iteration. See qrm_shift_spmat for more information.\nspfct: a sparse factorization object of type qrm_spfct.\nx: the approximate solution vector x := A†b = Aᵀy, the size of this vector is n+m, its values are overwritten by this function and it can be uninitialized when the function is called.\nb: the RHS vector of the linear system, the size of this vector is m.\nΔx: an auxiliary vector used to compute the solution, the size of this vector is n+m and can be uninitialized when the function is called.\ny: the vector used to compute y := (AAᵀ)†b, the size of this vector is n and can be uninitialized when the function is called.\nΔy: an auxiliary vector used to compute the solution, the size of this vector is n and can be uninitialized when the function is called.\n\nThe definition of the shifted_spmat may look odd at first sight. We use such a block matrix as an argument of qrm_golub_riley! because the QR factorization of this matrix has the property that RᵀR = AᵀA + αI which is the system we need to repeatedly solve from for the Golub-Riley iteration. \n\nThe Golub-Riley method works as follows:\n\nGiven A and b, choose a fixed regularization parameter α > 0 and let x := 0.\nAt each iteration, compute Δx := Aᵀ(AAᵀ + αI)⁻¹ (b - Ax), and update x := x + Δx.\n\nThis method has the advantage of only costing one QR-factorization of the qrm_shifted_spmat matrix. For more details, see  A. Dax and L. Eldén, Approximating minimum norm solutions of rank-deficient least squares problems, Numerical Linear Algebra with Applications, 5, pp. 79-99, 1998, DOI 10.1002/(SICI)1099-1506(199803/04)5:2<79::AID-NLA126>3.0.CO;2-4\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_golub_riley","page":"API","title":"QRMumps.qrm_golub_riley","text":"x = qrm_golub_riley(spmat, b)\n\nInput Arguments :\n\nspmat: the input matrix of the ill-conditionned system Ax = b.\nb: the RHS of the ill-conditionned system. \n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spmat_mv!","page":"API","title":"QRMumps.qrm_spmat_mv!","text":"qrm_spmat_mv!(spmat, alpha, x, beta, y; transp='n')\n\nThis subroutine performs a matrix-vector product of the type y = αAx + βy, y = αAᵀx + βy or y = αAᴴx + βy.\n\nInput Arguments :\n\nspmat: the input matrix.\nalpha, beta : the α and β scalars\nx: the x vector(s).\ny: the y vector(s).\ntransp: whether to multiply by A, Aᵀ or Aᴴ. Can be either 't', 'c' or 'n'.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_spmat_nrm","page":"API","title":"QRMumps.qrm_spmat_nrm","text":"qrm_spmat_nrm(spmat; ntype='f')\n\nThis routine computes the one-norm A_1, the infinity-norm x_infty or the two-norm x_2 of a matrix.\n\nInput Arguments :\n\nspmat: the input matrix.\nntype: the type of norm to be computed. It can be either 'i', '1' or 'f' for the infinity, one and Frobenius norms, respectively.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_vecnrm!","page":"API","title":"QRMumps.qrm_vecnrm!","text":"qrm_vecnrm!(x, nrm; ntype='2')\n\nThis routine computes the one-norm x_1, the infinity-norm x_infty or the two-norm x_2 of a vector.\n\nInput Arguments :\n\nx: the x vector(s).\nnrm: the computed norm(s). If x is a matrix this argument has to be a vector and each of its elements will contain the norm of the corresponding column of x.\nntype: the type of norm to be computed. It can be either 'i', '1' or '2' for the infinity, one and two norms, respectively.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_vecnrm","page":"API","title":"QRMumps.qrm_vecnrm","text":"nrm = qrm_vecnrm(x; ntype='2')\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_residual_norm!","page":"API","title":"QRMumps.qrm_residual_norm!","text":"qrm_residual_norm!(spmat, b, x, nrm; transp='n')\n\nThis function computes the scaled norm of the residual fracb - Ax_inftyb_infty + x_infty A_infty, i.e., the normwise backward error.\n\nInput Arguments :\n\nspmat: the input matrix.\nb: the right-hand side(s).\nx: the solution vector(s).\nnrm: the computed norm(s).\ntransp: whether to use A, Aᵀ or Aᴴ. Can be either 't', 'c' or 'n'.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_residual_norm","page":"API","title":"QRMumps.qrm_residual_norm","text":"nrm = qrm_residual_norm(spmat, b, x; transp='n')\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_residual_orth!","page":"API","title":"QRMumps.qrm_residual_orth!","text":"qrm_residual_orth!(spmat, r, nrm; transp='n')\n\nComputes the quantity fracA^T r_2r_2 which can be used to evaluate the quality of the solution of a least squares problem.\n\nInput Arguments :\n\nspmat: the input matrix.\nr: the residual(s).\nnrm: the computed norm(s).\ntransp: whether to use A, Aᵀ or Aᴴ. Can be either 't', 'c' or 'n'.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_residual_orth","page":"API","title":"QRMumps.qrm_residual_orth","text":"nrm = qrm_residual_orth(spmat, r; transp='n')\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_set","page":"API","title":"QRMumps.qrm_set","text":"qrm_set(str, val)\nqrm_set(spfct, str, val)\n\nSet control parameters that define the behavior of qr_mumps.\n\nInput Arguments :\n\nspfct: a sparse factorization object of type qrm_spfct.\nstr: a string describing the parameter to set.\nval: the parameter value.\n\n\n\n\n\n","category":"function"},{"location":"api/#QRMumps.qrm_get","page":"API","title":"QRMumps.qrm_get","text":"val = qrm_get(str)\nval = qrm_get(spfct, str)\n\nReturns the value of a control parameter or an information parameter.\n\nInput Arguments :\n\nspfct: a sparse factorization object of type qrm_spfct.\nstr: a string describing the parameter to get.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/ordering/","page":"Ordering","title":"Ordering","text":"QRMumps.jl supports different column orderings to reduce fill-in in the factors. By default, an ordering is chosen automatically by qr_mumps, but it is also possible to set the ordering manually. This ordering must always be set before the analysis phase.\n\nusing QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 1, 1, 2, 3, 3, 4, 4, 5, 5, 6, 7, 7]\njcn = [1, 3, 5, 2, 3, 5, 1, 4, 4, 5, 2, 1, 3]\nval = [1.0, 2.0, 3.0, 1.0, 1.0, 2.0, 4.0, 1.0, 5.0, 1.0, 3.0, 6.0, 1.0]\n\nA = sparse(irn, jcn, val, 7, 5)\nb = [22.0, 5.0, 13.0, 8.0, 25.0, 5.0, 9.0]\nx_star = [1.0, 2.0, 3.0, 4.0, 5.0]\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\n# Use the natural ordering (no permutation is used)\nqrm_set(spfct, \"qrm_ordering\", 1)\n\nqrm_analyse!(spmat, spfct)\nqrm_factorize!(spmat, spfct)\nz = qrm_apply(spfct, b, transp='t')\nx = qrm_solve(spfct, z)\n\nerror_norm = norm(x - x_star)\nr = A * x - b\noptimality_residual_norm = norm(A' * r)\n\n@printf(\"Error norm ‖x* - x‖ = %10.5e\\n\", error_norm)\n@printf(\"Optimality residual norm ‖Aᵀr‖ = %10.5e\\n\", optimality_residual_norm)\n\nusing QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 1, 1, 2, 3, 3, 4, 4, 5, 5, 6, 7, 7]\njcn = [1, 3, 5, 2, 3, 5, 1, 4, 4, 5, 2, 1, 3]\nval = [1.0+im, 2.0-im, 3.0+im, 1.0-im, 1.0+im, 2.0-im, 4.0+im, 1.0-im, 5.0+im, 1.0-im, 3.0+im, 6.0-im, 1.0+im]\n\nA = sparse(irn, jcn, val, 7, 5)\nb = [1.0+im, 2.0+im, 3.0+im, 4.0+im, 5.0+im, 6.0+im, 7.0+im]\nz = copy(b)\nx = zeros(ComplexF64, 5)\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\n# Provide your own column permutation\npermutation = Cint[i for i = 5:-1:1]\nqrm_user_permutation!(spfct, permutation)\nqrm_set(spfct, \"qrm_ordering\", 2)\n\nqrm_analyse!(spmat, spfct)\nqrm_factorize!(spmat, spfct)\nqrm_apply!(spfct, z, transp='c')\nqrm_solve!(spfct, z, x)\n\nr = A * x - b\noptimality_residual_norm = norm(A' * r)\n\n@printf(\"Optimality residual norm ‖Aᵀr‖ = %10.5e\\n\", optimality_residual_norm)\n\nusing QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 2, 1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5]\njcn = [1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5]\nval = [4.0, 1.0, 1.0, 4.0, 1.0, 1.0, 4.0, 1.0, 1.0, 4.0, 1.0, 1.0, 4.0]\n\nA = sparse(irn, jcn, val, 5, 5)\nA_U = triu(A)\nb = [5.0, 6.0, 6.0, 6.0, 5.0]\nx_star = [1.0, 1.0, 1.0, 1.0, 1.0]\n\nqrm_init()\n\nspmat = qrm_spmat_init(A_U, sym=true)\nspfct = qrm_spfct_init(spmat)\n\n# Compute a column permutation with COLAMD\nqrm_set(spfct, \"qrm_ordering\", 3)\n\nqrm_analyse!(spmat, spfct)\nqrm_factorize!(spmat, spfct)\nz = qrm_solve(spfct, b, transp='t')\nx = qrm_solve(spfct, z)\n\nerror_norm = norm(x - x_star)\nresidual_norm = norm(A * x - b)\n\n@printf(\"Error norm ‖x* - x‖ = %10.5e\\n\", error_norm)\n@printf(\"Residual norm ‖Ax - b‖ = %10.5e\\n\", residual_norm)\n\nusing QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 1, 1, 2, 2, 3]\njcn = [1, 2, 3, 2, 3, 3]\nval = [7.0, im, -5im, 8.0, 5.0, 10.0]\n\nA = Hermitian(sparse(irn, jcn, val, 3, 3), :U)\nb = [11.0-6im, 32.0+12im, 35.0+20im]\nx_star = [1.0+im, 2.0+im, 3.0+im]\nx = copy(b)\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\n# Compute a column permutation with METIS\nqrm_set(spfct, \"qrm_ordering\", 4)\n\nqrm_analyse!(spmat, spfct)\nqrm_factorize!(spmat, spfct)\n\nqrm_solve!(spfct, x, x, transp='c')\nqrm_solve!(spfct, x, x)\n\nerror_norm = norm(x - x_star)\nresidual_norm = norm(A * x - b)\n\n@printf(\"Error norm ‖x* - x‖ = %10.5e\\n\", error_norm)\n@printf(\"Residual norm ‖Ax - b‖ = %10.5e\\n\", residual_norm)","category":"section"},{"location":"tutorials/qless/","page":"Semi-normal equations and Q-less factorization","title":"Semi-normal equations and Q-less factorization","text":"# The Q-less QR factorization may be used to solve the least-norm problem\n#\n#   minimize ‖x‖  subject to  Ax=b\n#\n# while saving storage because Q is not formed.\n# Thus it is appropriate for large problems where storage is at a premium.\n# The normal equations of the second kind AAᵀy = b are the optimality conditions of the least-norm problems, where x = Aᵀy.\n# If Aᵀ = QR, they can be equivalently written RᵀRy = b.\n#\n# The stability of this procedure is comparable to the method that uses Q---see\n#\n# C. C. Paige, An error analysis of a method for solving matrix equations,\n# Mathematics of Computations, 27, pp. 355-359, 1973, DOI 10.2307/2005623.\n\nusing LinearAlgebra, Printf, SparseArrays\nusing QRMumps\n\n# Initialize data\nm, n = 5, 7\nirn = [1, 3, 5, 2, 3, 5, 1, 4, 4, 5, 2, 1, 3]\njcn = [1, 1, 1, 2, 3, 3, 4, 4, 5, 5, 6, 7, 7]\nval = [1.0, 2.0, 3.0, 1.0, 1.0, 2.0, 4.0, 1.0, 5.0, 1.0, 3.0, 6.0, 1.0]\n\nA = sparse(irn, jcn, val, m, n)\nb = [40.0, 10.0, 44.0, 98.0, 87.0]\nx_star = [16.0, 1.0, 10.0, 3.0, 19.0, 3.0, 2.0]\ny₁ = zeros(n)\ny = zeros(m)\nx = zeros(n)\n\ne = zeros(n)\nr = zeros(m)\n\n# Initialize QRMumps\nqrm_init()\n\n# Initialize data structures\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\n# Specify that we want the Q-less QR factorization\nqrm_set(spfct, \"qrm_keeph\", 0)\n\n# Perform symbolic analysis of Aᵀ and factorize Aᵀ = QR\nqrm_analyse!(spmat, spfct; transp='t')\nqrm_factorize!(spmat, spfct, transp='t')\n\n# Solve RᵀR y = b in two steps:\n# 1. Solve Rᵀy₁ = b  \nqrm_solve!(spfct, b, y₁; transp='t')\n\n# 2. Solve Ry = y₁\nqrm_solve!(spfct, y₁, y; transp='n')\n\n\n# Compute the least norm solution of Ax = b\nx .= A'*y\n\n# Compute error norm and residual norm\n@. e = x - x_star\nerror_norm = norm(e)\n\nmul!(r, A, x)\n@. r = b - r\nresidual_norm = norm(r)\n\n@printf(\"Error norm ‖x* - x‖ = %7.1e\\n\", error_norm)\n@printf(\"Residual norm ‖b - Ax‖ = %7.1e\\n\", residual_norm)\n\n# Alternatively, you can use `qrm_min_norm_semi_normal!`, which performs these steps automatically\n\n# Initialize data structures\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\n# Solve the minimum-norm problem\nqrm_min_norm_semi_normal!(spmat, spfct, b, x, y₁, y)\n\n# Compute error norm and residual norm\n@. e = x - x_star\nerror_norm = norm(e)\n\nmul!(r, A, x)\n@. r = b - r\nresidual_norm = norm(r)\n\n@printf(\"Error norm (qrm function) ‖x* - x‖ = %7.1e\\n\", error_norm)\n@printf(\"Residual norm (qrm function) ‖b - Ax‖ = %7.1e\\n\", residual_norm)\n\n# The Q-less QR factorization may be used to solve the least-square problem\n#\n#   minimize ‖Ax - b‖\n#\n# while saving storage because Q is not formed.\n# Thus it is appropriate for large problems where storage is at a premium.\n# The normal equations AᵀAx = Aᵀb are the optimality conditions of the least-squares problem.\n# If A = QR, they can be equivalently written RᵀRx = Aᵀb.\n#\n# This procedure is backward stable if we perform one step of iterative refinement---see\n#\n# Å. Björck, Stability analysis of the method of seminormal equations for linear least squares problems,\n# Linear Algebra and its Applications, 88–89, pp. 31-48, 1987, DOI 10.1016/0024-3795(87)90101-7.\n\nusing LinearAlgebra, Printf, SparseArrays \nusing QRMumps\n\n# Initialize data\nm, n = 7, 5\nirn = [1, 1, 1, 2, 3, 3, 4, 4, 5, 5, 6, 7, 7]\njcn = [1, 3, 5, 2, 3, 5, 1, 4, 4, 5, 2, 1, 3]\nval = [1.0, 2.0, 3.0, 1.0, 1.0, 2.0, 4.0, 1.0, 3.0, 1.0, 3.0, 2.0, 1.0]\n\nA = sparse(irn, jcn, val, m, n)\nb = [22.0, 2.0, 13.0, 8.0, 17.0, 6.0, 5.0]\nx_star = [1.0, 2.0, 3.0, 4.0, 5.0]\n\nz = zeros(n)\nx₁ = zeros(m)\nx = zeros(n)\n\nr = zeros(m)\nAr = zeros(n)\ne = zeros(n) \nΔx₁ = zeros(m)\nΔx = zeros(n)\n\n# Initialize QRMumps\nqrm_init()\n\n# Initialize data structures\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\n# Specify that we want the Q-less QR factorization\nqrm_set(spfct, \"qrm_keeph\", 0)\n\n# Perform symbolic analysis of A and factorize A = QR\nqrm_analyse!(spmat, spfct)\nqrm_factorize!(spmat, spfct)\n\n# Compute the RHS of the semi-normal equations\nmul!(z, A', b)\n\n# Solve RᵀR x = z = Aᵀb in two steps:\n# 1. Solve Rᵀx₁ = z  \nqrm_solve!(spfct, z, x₁; transp = 't')\n\n# 2. Solve Rx = x₁\nqrm_solve!(spfct, x₁, x; transp = 'n')\n\n# Compute errors\n@. e = x - x_star\nerror_norm = norm(e)\n\n# Compute the normal equations residual in two steps to prevent allocating memory:\n# 1. Compute r = b - Ax\nmul!(r, A, x)\n@. r = b - r\n\n# 2. Compute Aᵀr = Aᵀ(b - A*x)\nmul!(Ar, A', r)\nAresidual_norm = norm(Ar)\n\n@printf(\"Error norm ‖x* - x‖ = %7.1e\\n\", error_norm)\n@printf(\"Normal equations residual norm ‖Aᵀ(Ax - b)‖= %7.1e\\n\", Aresidual_norm)\n\n# As such, this method is not backward stable and we need to add an iterative refinement step:                                                          \n# For this, we compute the least-squares solution Δx of min ‖Aᵀr - AΔx‖, where r is the residual r = b - A*x.\n# We then update x := x + Δx\n\n# Solve the semi-normal equations as before\nqrm_solve!(spfct, Ar, Δx₁; transp='t')\nqrm_solve!(spfct, Δx₁, Δx; transp='n')\n\n# Update the least squares solution\n@. x = x + Δx\n\n# Compute errors as before\n@. e = x - x_star\nerror_norm = norm(e)\n\nmul!(r, A, x)\n@. r = b - r\nmul!(Ar, A', r)\nAresidual_norm = norm(Ar)\n\n@printf(\"Error norm (iterative refinement step) ‖x* - x‖ = %7.1e\\n\", error_norm)\n@printf(\"Normal equations residual norm (iterative refinement step) ‖Aᵀ(Ax - b)‖= %7.1e\\n\", Aresidual_norm)\n\n# Alternatively, you can use `qrm_least_squares_semi_normal!`, which performs these steps automatically\n\n# Initialize data structures\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\n# Solve the least-squares problem\nqrm_least_squares_semi_normal!(spmat, spfct, b, x, z, Δx, x₁)\n\n# Compute errors\n@. e = x - x_star\nerror_norm = norm(e)\n\nmul!(r, A, x)\n@. r = b - r\n\nmul!(Ar, A', r)\nAresidual_norm = norm(Ar)\n\n@printf(\"Error norm (qrm function) ‖x* - x‖ = %7.1e\\n\", error_norm)\n@printf(\"Normal equations residual norm (qrm function) ‖Aᵀ(Ax - b)‖= %7.1e\\n\", Aresidual_norm)","category":"section"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"experimental/","page":"-","title":"-","text":"qrm_ngpu: integer specifying the number of GPUs to use for the subsequent qr_mumps calls. It is an argument to the qrm_init routine. Default is 0.\nqrm_pinth: an integer value to control memory pinning when GPUs are used: all frontal matrices whose size (min(rows,cols)) is bigger than this value will be pinned.\n\nNote that it is possible to use multiple streams per GPU; this can be controlled through the StarPU STARPU_NWORKER_PER_CUDA environment variable.","category":"section"},{"location":"experimental/#**GPU-streams**","page":"-","title":"GPU streams","text":"When GPUs are used, it can be helpful (and it usually is) to use multiple streams per GPU to allow a single GPU to execute multiple tasks concurrently. Using multiple GPU streams is especially beneficial to achieve high GPU occupancy when a relatively small block size mb is chosen to prevent CPU starvation. This can be controlled through the STARPU_NWORKER_PER_CUDA StarPU environment variable. By default one stream is active per GPU device and higher performance can be commonly achieved with values of 2 up to 20.","category":"section"},{"location":"tutorials/ln/","page":"Least-norm problems","title":"Least-norm problems","text":"using QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 5]\njcn = [3, 5, 7, 1, 4, 6, 2, 6, 5, 6, 3, 4, 7]\nval = [2.0, 3.0, 5.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0]\n\nA = sparse(irn, jcn, val, 5, 7)\nb = [56.0, 21.0, 16.0, 22.0, 25.0]\nx_star = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nx = qrm_min_norm(spmat, b)\n\nerror_norm = norm(x - x_star)\nresidual_norm = norm(A * x - b)\n\n@printf(\"Error norm ‖x* - x‖ = %10.5e\\n\", error_norm)\n@printf(\"Residual norm ‖Ax - b‖ = %10.5e\\n\", residual_norm)\n\nusing QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 5]\njcn = [3, 5, 7, 1, 4, 6, 2, 6, 5, 6, 3, 4, 7]\nval = [2.0, 3.0, 5.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0]\n\nA = sparse(irn, jcn, val, 5, 7)\nb = [56.0, 21.0, 16.0, 22.0, 25.0]\nx_star = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\nqrm_analyse!(spmat, spfct, transp='t')\nqrm_factorize!(spmat, spfct, transp='t')\nz = qrm_solve(spfct, b, transp='t')\nx = qrm_apply(spfct, z)\n\nerror_norm = norm(x - x_star)\nresidual_norm = norm(A * x - b)\n\n@printf(\"Error norm ‖x* - x‖ = %10.5e\\n\", error_norm)\n@printf(\"Residual norm ‖Ax - b‖ = %10.5e\\n\", residual_norm)\n\nusing QRMumps, LinearAlgebra, SparseArrays, Printf\n\nirn = [1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 5]\njcn = [3, 5, 7, 1, 4, 6, 2, 6, 5, 6, 3, 4, 7]\nval = [1.0-im, 2.0+im, 3.0-im, 1.0+im, 1.0-im, 2.0+im, 4.0-im, 1.0+im, 5.0-im, 1.0+im, 3.0-im, 6.0+im, 1.0-im]\n\nA = sparse(irn, jcn, val, 5, 7)\nb = [1.0+im, 2.0+im, 3.0+im, 4.0+im, 5.0+im]\nx = zeros(ComplexF64, 7)\n\nqrm_init()\n\nspmat = qrm_spmat_init(A)\nspfct = qrm_spfct_init(spmat)\n\nqrm_analyse!(spmat, spfct, transp='c')\nqrm_factorize!(spmat, spfct, transp='c')\nqrm_solve!(spfct, b, x, transp='c')\nqrm_apply!(spfct, x)\n\nresidual_norm = norm(A * x - b)\n\n@printf(\"Residual norm ‖Ax - b‖ = %10.5e\\n\", residual_norm)","category":"section"},{"location":"#Introduction","page":"Introduction","title":"Introduction","text":"This package provides a Julia interface to qr_mumps, a software for solving sparse linear systems on multicore computers. qr_mumps implements a direct solution method based on the QR or Cholesky factorization of the input matrix.  Therefore, it is suited to solving sparse least-squares problems, to computing the minimum-norm solution of sparse, underdetermined problems and to solving symmetric, positive-definite sparse linear systems. It can obviously be used for solving square unsymmetric problems in which case the stability provided by the use of orthogonal transformations comes at the cost of a higher operation count with respect to solvers based on, e.g., the LU factorization such as MUMPS. It supports real and complex, single or double precision arithmetic.\n\nAs in all the sparse, direct solvers, the solution is achieved in three distinct phases:\n\nAnalysis\nIn this phase an analysis of the structural properties of the input matrix is performed in preparation for the numerical factorization phase. This includes computing a column permutation which reduces the amount of fill-in coefficients (i.e., nonzeroes introduced by the factorization). This step does not perform any floating-point operation and is, thus, commonly much faster than the factorization and solve (depending on the number of right-hand sides) phases.\nFactorization\nAt this step, the actual QR or Cholesky factorization is computed. This step is the most computationally intense and, therefore, the most time consuming.\nSolution\nOnce the factorization is done, the factors can be used to compute the solution of the problem through two operations:\nSolve : this operation computes the solution of the triangular system Rx=b or Rᵀx=b;\nApply : this operation applies the Q orthogonal matrix to a vector, i.e., y=Qx or y=Qᵀx. \n\nThese three steps have to be done in order but each of them can be performed multiple times. If, for example, the problem has to be solved against multiple right-hand sides (not all available at once), the analysis and factorization can be done only once while the solution is repeated for each right-hand side. By the same token, if the coefficients of a matrix are updated but not its structure, the analysis can be performed only once for multiple factorization and solution steps.\n\nqr_mumps is based on the multifrontal factorization method. This method was first introduced by Duff and Reid as a method for the factorization of sparse, symmetric linear systems and, since then, has been the object of numerous studies and the method of choice for several, high-performance, software packages such as MUMPS and UMFPACK.","category":"section"},{"location":"optional_features/#Optional-features","page":"Optional features","title":"Optional features","text":"The following features of the qr_mumps software are currently unavailable in the Julia interface if the package is installed through Yggdrasil. If a custom qr_mumps install is used which has support for the StarPU runtime, these features can be accessed, although they have not been thoroughly tested within Julia.","category":"section"},{"location":"optional_features/#**Multithreading**","page":"Optional features","title":"Multithreading","text":"qr_mumps is a parallel, multithreaded software based on the StarPU runtime system and it currently supports multicore or, more generally, shared memory multiprocessor computers. qr_mumps does not run on distributed memory (e.g. clusters) parallel computers. Parallelism is achieved through a decomposition of the workload into fine-grained computational tasks which basically correspond to the execution of a BLAS or LAPACK operation on a blocks. It is strongly recommended to use sequential BLAS and LAPACK libraries and let qr_mumps have full control of the parallelism. The granularity of the tasks is controlled by the qrm_mb and qrm_nb parameters which set the block size for partitioning internal data. Smaller values mean more parallelism; however, because this blocking factor is an upper-bound for the granularity of operations (or, more precisely for the granularity of calls to BLAS and LAPACK routines), it is recommended to choose reasonably large values in order to achieve high efficiency.","category":"section"},{"location":"optional_features/#**GPU-acceleration**","page":"Optional features","title":"GPU acceleration","text":"qr_mumps can leverage the computing power of Nvidia GPU, commonly available on modern super-computing systems, to accelerate the solution of linear systems, especially large size ones. The use of GPUs is achieved through the StarPU runtime.","category":"section"}]
}
